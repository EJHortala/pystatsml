{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network\n",
    "\n",
    "Sources:\n",
    "\n",
    "Deep learning\n",
    "\n",
    "- [cs231n.stanford.edu](http://cs231n.stanford.edu/)\n",
    "\n",
    "CNN\n",
    "\n",
    "- [Stanford cs231n](http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "Pytorch\n",
    "\n",
    "- [WWW tutorials](https://pytorch.org/tutorials/)\n",
    "- [github tutorials](https://github.com/pytorch/tutorials)\n",
    "- [github examples](https://github.com/pytorch/examples)\n",
    "\n",
    "MNIST and pytorch:\n",
    "\n",
    "- [MNIST nextjournal.com/gkoehler/pytorch-mnist](https://nextjournal.com/gkoehler/pytorch-mnist)\n",
    "- [MNIST github/pytorch/examples](https://github.com/pytorch/examples/tree/master/mnist)\n",
    "- [MNIST kaggle](https://www.kaggle.com/sdelecourt/cnn-with-pytorch-for-mnist)\n",
    "\n",
    "\n",
    "## Architectures\n",
    "\n",
    "\n",
    "### Architecures general guidelines\n",
    "\n",
    "- ConvNets stack CONV,POOL,FC layers\n",
    "- Trend towards smaller filters and deeper architectures\n",
    "- Trend towards getting rid of POOL/FC layers (just CONV)\n",
    "- Historically architectures looked like [(CONV-RELU) x N POOL?] x M (FC-RELU) x K, SOFTMAX where N is usually up to ~5, M is large, 0 <= K <= 2.\n",
    "- but recent advances such as ResNet/GoogLeNet have challenged this paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "WD = os.path.join(tempfile.gettempdir(), \"dl_cnn_mnist_pytorch\")\n",
    "os.makedirs(WD, exist_ok=True)\n",
    "os.chdir(WD)\n",
    "print(\"Working dir is:\", os.getcwd())\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "random_seed = 1\n",
    "no_cuda = True\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: MNIST Handwritten Digit Recognition in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(batch_size_train, batch_size_test):\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size_train, shuffle=True)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size_test, shuffle=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = load_mnist(batch_size_train, batch_size_test)\n",
    "data_shape = train_loader.dataset.data.shape[1:]\n",
    "D_in = np.prod(data_shape)\n",
    "D_out = len(train_loader.dataset.targets.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch, device, log_interval=10, batch_max=np.inf):\n",
    "    train_losses, train_counter = list(), list()\n",
    "    # epoch = 1; log_interval=10; train_losses=[]; train_counter=[]\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over minibatch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if batch_idx > batch_max:\n",
    "            break\n",
    "        # batch_idx, (data, target) = next(enumerate(train_loader))\n",
    "        # print(data.shape)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Forward\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "    \n",
    "        # Bakward\n",
    "        loss.backward()\n",
    "\n",
    "        # Update params\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track losses\n",
    "        train_losses.append(loss.item())\n",
    "        train_counter.append(data.shape[0]) # (batch_idx * data.shape[0]) + ((epoch-1)*len(train_loader.dataset)))\n",
    "\n",
    "        # Save model\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            torch.save(model.state_dict(), 'models/mod-%s.pth' % model.__class__.__name__)\n",
    "            torch.save(optimizer.state_dict(), 'models/mod-%s_opt-%s.pth' % (model.__class__.__name__, optimizer.__class__.__name__))\n",
    "\n",
    "    return model, train_losses, train_counter\n",
    "\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    output, pred, target = list(), list(), list()\n",
    "\n",
    "    # Iterate over mini-batches\n",
    "    with torch.no_grad():\n",
    "        for data, target_ in test_loader:\n",
    "            # batch_idx, (data, target) = next(enumerate(test_loader))\n",
    "            # print(target_.shape)\n",
    "            data, target_ = data.to(device), target_.to(device) # target.shape == 1000\n",
    "            output_ = model(data) # output.shape == (1000, 10)\n",
    "            \n",
    "            # Compute loss\n",
    "            test_loss += F.nll_loss(output_, target_, reduction='sum').item() # sum up batch loss\n",
    "            pred_ = output_.argmax(dim=1) # get the index of the max log-probability\n",
    "            \n",
    "            # An correct classification\n",
    "            correct += pred_.eq(target_.view_as(pred_)).sum().item() # view_as(other): View this tensor as the same size as other\n",
    "\n",
    "            # Track output, class-prediction and true target\n",
    "            output.append(output_)\n",
    "            pred.append(pred_)\n",
    "            target.append(target_)\n",
    "\n",
    "    output = torch.cat(output)\n",
    "    pred = torch.cat(pred)\n",
    "    target = torch.cat(target)\n",
    "    assert pred.eq(target.view_as(pred)).sum().item() == correct\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return pred, output, target, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN models\n",
    "\n",
    "#### LeNet-5\n",
    "\n",
    "Here we implement LeNet-5 with relu activation\n",
    "[Source](https://github.com/bollakarthikeya/LeNet-5-PyTorch/blob/master/lenet5_cpu.py)\n",
    "\n",
    "<img src=\"figures/LeNet_Original_Image.jpg\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network (LeNet-5)  \n",
    "class LeNet5(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self):   \n",
    "        super(LeNet5, self).__init__()\n",
    "        # Convolution (In LeNet-5, 32x32 images are given as input. Hence padding of 2 is done below)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "        # Max-pooling\n",
    "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # Convolution\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
    "        # Max-pooling\n",
    "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = torch.nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)        # convert matrix with 84 features to a matrix of 10 features (columns)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.conv1(x))  \n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_1(x)\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_2(x)\n",
    "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
    "        # read through https://stackoverflow.com/a/42482819/7551231\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        # FC-1, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        # FC-2, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        # FC-3\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Increase depth on conv layer, remove one FC, simplify with no padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5bis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5bis, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1) \n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(50*4*4, 120) # 500\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # 20 x 24x24 # No padding, loose 4, since kernel size = 5\n",
    "        x = F.max_pool2d(x, 2, 2) # 20 x 12x12\n",
    "        x = F.relu(self.conv2(x)) # 50 x 8x8  # No padding, loose 4, since kernel size = 5\n",
    "        x = F.max_pool2d(x, 2, 2) # 50 x 4x4\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network (LeNet-5)  \n",
    "class ConvNet2Block(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self):   \n",
    "        super(ConvNet2Block, self).__init__()\n",
    "\n",
    "        # Conv block 1\n",
    "        self.conv11 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv12 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.max_pool_1 = nn.MaxPool2d(kernel_size=2) # output 16*14*14\n",
    "\n",
    "        # Conv block 2\n",
    "        self.conv21 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv22 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=2) # output 32*7*7\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(32*7*7, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv11(x))\n",
    "        x = nn.functional.relu(self.conv12(x))\n",
    "        x = self.max_pool_1(x)\n",
    "\n",
    "        x = torch.nn.functional.relu(self.conv21(x))\n",
    "        x = torch.nn.functional.relu(self.conv22(x))\n",
    "        x = self.max_pool_2(x)\n",
    "        \n",
    "        # first flatten 'max_pool_2_out' to contain 32*7*7 columns\n",
    "        # read through https://stackoverflow.com/a/42482819/7551231\n",
    "        x = x.view(-1, 32*7*7)\n",
    "        # FC-1, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LeNet5() # 98.4% with 61706 parameters\n",
    "#model = LeNet5bis() # 98.7% with 122900 parameters\n",
    "model = ConvNet2Block() # % with  205858 parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Explore the model\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)\n",
    "\n",
    "print(\"Total number of parameters =\", np.sum([np.prod(parameter.shape) for parameter in model.parameters()]))\n",
    "    \n",
    "train_losses, train_counter, test_losses, test_counter = [], [], [], []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Train\n",
    "    model, train_losses_, train_counter_ = train(model, train_loader, optimizer, epoch, device, log_interval)\n",
    "    train_losses += train_losses_\n",
    "    train_counter += train_counter_\n",
    "    \n",
    "    # Test\n",
    "    pred, output, target, test_loss = test(model, test_loader, device)\n",
    "    test_counter.append(np.sum(train_counter))\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    # Train accuracy\n",
    "    pred_train, output_train, target_train, loss_train = test(model, train_loader, device)\n",
    "    print(\"Train accuracy = {:.1f}%\".format((target_train == pred_train).sum().item() * 100. / len(target_train)))\n",
    "    print(\"Test accuracy = {:.1f}%\".format((target == pred).sum().item() * 100. / len(target)))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(np.cumsum(train_counter), train_losses, color='blue')\n",
    "plt.plot(test_counter, test_losses, \"or\")\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization with two dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetDropOut(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
