{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MNIST Handwritten Digit Recognition in PyTorch\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [MNIST nextjournal.com/gkoehler/pytorch-mnist](https://nextjournal.com/gkoehler/pytorch-mnist)\n",
    "- [MNIST github/pytorch/examples](https://github.com/pytorch/examples/tree/master/mnist)\n",
    "- [MNIST kaggle](https://www.kaggle.com/sdelecourt/cnn-with-pytorch-for-mnist)\n",
    "\n",
    "Convert to jupyter:\n",
    "```\n",
    "sphx_glr_python_to_jupyter.py  dl_cnn_mnist_pytorch.py\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters we'll be using for the experiment.\n",
    "\n",
    "Here the number of epochs defines how many times we'll\n",
    "loop over the complete training dataset, while learning_rate and momentum are hyperparameters\n",
    "for the optimizer we'll be using later on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "WD = os.path.join(tempfile.gettempdir(), \"dl_cnn_mnist_pytorch\")\n",
    "os.makedirs(WD, exist_ok=True)\n",
    "os.chdir(WD)\n",
    "print(\"Working dir is:\", os.getcwd())\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "n_epochs = 2\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "random_seed = 1\n",
    "no_cuda = True\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(batch_size_train, batch_size_test):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size_train, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=batch_size_test, shuffle=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = load_mnist(batch_size_train, batch_size_test)\n",
    "data_shape = train_loader.dataset.data.shape[1:]\n",
    "D_in = np.prod(data_shape)\n",
    "D_out = len(train_loader.dataset.targets.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at some examples. We'll use the test_loader for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx, (example_data, example_targets) = next(enumerate(test_loader))\n",
    "print(example_data.shape, example_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So one test data batch is a tensor of shape: . This means we have 1000 examples of 28x28 pixels in grayscale\n",
    "(i.e. no rgb channels, hence the one). We can plot some of them using matplotlib.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_data_label_prediction(data, y_true, y_pred=None, shape=(2, 3)):\n",
    "    y_pred = [None] * len(y_true) if y_pred is None else y_pred\n",
    "    fig = plt.figure()\n",
    "    for i in range(np.prod(shape)):\n",
    "        plt.subplot(*shape, i+1)\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(data[i][0], cmap='gray', interpolation='none')\n",
    "        plt.title(\"True: {} Pred: {}\".format(y_true[i], y_pred[i]))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "    return fig\n",
    "\n",
    "show_data_label_prediction(data=example_data, y_true=example_targets, y_pred=None, shape=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax Classifier (Multinomial Logistic Regression)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super(TwoLayerMLP, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_in, d_hidden)\n",
    "        self.linear2 = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, self.d_in)\n",
    "        X = self.linear1(X)\n",
    "        return F.log_softmax(self.linear2(X), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP\n",
    "\n",
    "For MNIST, D_in=784, 784*(250+1) + 250*(100+1) + 100*(10+1) = 222 360 parameters to train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, 250)\n",
    "        self.linear2 = nn.Linear(250, 100)\n",
    "        self.linear3 = nn.Linear(100, D_out)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, D_in)\n",
    "        X = F.relu(self.linear1(X))\n",
    "        X = F.relu(self.linear2(X))\n",
    "        X = self.linear3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    "\n",
    "class MLPDropOut(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in, 50)\n",
    "        self.fc1_drop = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc2_drop = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(50, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, D_in)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_drop(x)\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "#mlp = MLP()\n",
    "#print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Models\n",
    "We'll use two 2-D convolutional layers followed by two fully-connected (or linear) layers. As activation function\n",
    "we'll choose rectified linear units (ReLUs in short)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularization with two dropout layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetDropOut(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "\n",
    "First we want to make sure our network is in training mode.\n",
    "Then we iterate over all training data once per epoch.\n",
    "Loading the individual batches is handled by the DataLoader.\n",
    "First we need to manually set the gradients to zero using `optimizer.zero_grad()` since PyTorch by default\n",
    "accumulates gradients.\n",
    "We then produce the output of our network (forward pass) and compute a negative log-likelihodd loss between the\n",
    "output and the ground truth label.\n",
    "\n",
    "The backward() call we now collect a new set of gradients which we propagate back into each of the network's\n",
    "parameters using optimizer.step().\n",
    "For more detailed information about the inner workings of PyTorch's automatic gradient system,\n",
    "see the official docs for autograd (highly recommended).\n",
    "\n",
    "We'll also keep track of the progress with some printouts. In order to create a nice training curve later on\n",
    "we also create two lists for saving training and testing losses.\n",
    "On the x-axis we want to display the number of training examples the network has seen during training.\n",
    "\n",
    "Neural network modules as well as optimizers have the ability to save and load their internal state using\n",
    "`.state_dict()`. With this we can continue training from previously saved state dicts if needed - we'd just need\n",
    "to call `.load_state_dict(state_dict)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch, device, log_interval=10):\n",
    "    train_losses, train_counter = list(), list()\n",
    "    # epoch = 1; log_interval=10; train_losses=[]; train_counter=[]\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # batch_idx, (data, target) = next(enumerate(train_loader))\n",
    "        # print(data.shape)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_counter.append(data.shape[0]) # (batch_idx * data.shape[0]) + ((epoch-1)*len(train_loader.dataset)))\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "            torch.save(model.state_dict(), 'models/mod-%s.pth' % model.__class__.__name__)\n",
    "            torch.save(optimizer.state_dict(), 'models/mod-%s_opt-%s.pth' % (model.__class__.__name__, optimizer.__class__.__name__))\n",
    "\n",
    "    return model, train_losses, train_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loop. Here we sum up the test loss and keep track of correctly classified digits to compute the accuracy of\n",
    "the network.\n",
    "Using the context manager no_grad() we can avoid storing the computations done producing the output of our network\n",
    "in the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    output, pred, target = list(), list(), list()\n",
    "    with torch.no_grad():\n",
    "        for data, target_ in test_loader:\n",
    "            # batch_idx, (data, target) = next(enumerate(test_loader))\n",
    "            # print(target_.shape)\n",
    "            data, target_ = data.to(device), target_.to(device) # target.shape == 1000\n",
    "            output_ = model(data) # output.shape == (1000, 10)\n",
    "            test_loss += F.nll_loss(output_, target_, reduction='sum').item() # sum up batch loss\n",
    "            pred_ = output_.argmax(dim=1) # get the index of the max log-probability\n",
    "            correct += pred_.eq(target_.view_as(pred_)).sum().item() # view_as(other): View this tensor as the same size as other\n",
    "            output.append(output_)\n",
    "            pred.append(pred_)\n",
    "            target.append(target_)\n",
    "\n",
    "    output = torch.cat(output)\n",
    "    pred = torch.cat(pred)\n",
    "    target = torch.cat(target)\n",
    "    assert pred.eq(target.view_as(pred)).sum().item() == correct\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return pred, output, target, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Initialize the network and the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  If we were using a GPU for training, we should have also sent the network parameters to the GPU\n",
    "model = TwoLayerMLP(D_in, 50, D_out)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to run the training! We'll manually add a test() call before we loop over n_epochs to evaluate our model with\n",
    "randomly initialized parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, output, target, test_loss = test(model, test_loader, device)\n",
    "print(\"Test accuracy = {}%\".format((target == pred).sum() * 100. / len(target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train one epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_losses, train_counter = train(model, train_loader, optimizer, 1, device)\n",
    "pred, output, target, test_loss = test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Model's Performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test accuracy = {}%\".format((target == pred).sum() * 100. / len(target)))\n",
    "test_counter, test_losses = [len(train_loader.dataset)], [test_loss]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(np.cumsum(train_counter), train_losses, '-b',\n",
    "         np.cumsum(test_counter), test_losses, \"or\")\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's again look at a few examples as we did earlier and compare the model's output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  output = model(example_data)\n",
    "y_pred = output.argmax(dim=1)\n",
    "\n",
    "show_data_label_prediction(data=example_data, y_true=example_targets, y_pred=y_pred, shape=(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some missclassified images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = example_targets != y_pred\n",
    "print(\"Nb errors = {}, (rate = {:.2f}%)\".format(errors.sum(), 100 * errors.sum().item() / len(errors)))\n",
    "err_idx = np.where(errors)\n",
    "show_data_label_prediction(data=example_data[err_idx], y_true=example_targets[err_idx], y_pred=y_pred[err_idx], shape=(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerMLP(D_in, 50, D_out)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "model.load_state_dict(torch.load('models/mod-%s.pth' % model.__class__.__name__))\n",
    "optimizer.load_state_dict(torch.load('models/mod-%s_opt-%s.pth' % (model.__class__.__name__, optimizer.__class__.__name__)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue training from checkpoints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2, n_epochs + 1):\n",
    "#for epoch in range(n_epochs+1, n_epochs + 5):\n",
    "#for epoch in range(n_epochs + 5, n_epochs + 10):\n",
    "    model, train_losses_, train_counter_ = train(model, train_loader, optimizer, epoch, device, log_interval)\n",
    "    train_losses += train_losses_\n",
    "    train_counter += train_counter_\n",
    "    pred, output, target, test_loss = test(model, test_loader, device)\n",
    "    test_counter.append(len(train_loader.dataset))\n",
    "    test_losses.append(test_loss)\n",
    "    print(\"Test accuracy = {:.1f}%\".format((target == pred).sum().item() * 100. / len(target)))\n",
    "    #test(cont_mod, test_loader, epoch, device, test_losses)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(np.cumsum(train_counter), train_losses, color='blue')\n",
    "plt.plot(np.cumsum(test_counter), test_losses, \"or\")\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize coeficients map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [layer for layer in model.modules()]\n",
    "l = layers[0]\n",
    "weights = [p for p in l.parameters()]\n",
    "print([w.shape for w in weights])\n",
    "w = weights[0].detach().numpy()\n",
    "# torch.Size([10, 784]) => 10 x 1 x 28 x 28\n",
    "ima = np.concatenate([w[i].reshape(1, 1, 28, 28) for i in range(10)])\n",
    "ima = w.reshape(10, 1, 28, 28)\n",
    "show_data_label_prediction(data=ima, y_true=np.arange(10), y_pred=None, shape=(2, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
